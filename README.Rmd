---
output:
  md_document:
    variant: markdown_github
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "inst/README-figures/README-",
  dpi = 124
)
```

# gamtree: Generalized additive model (GAM) trees

Functions for detection and identification of subgroups in GAMs. The GAM can comprise any number of smooth and parametric terms, and the subgroups of interest may differ in terms of the effect of a single or multiple predictors. The effects of parameters of interest for the GAM, and for the subgroup detection in specific, can be flexibly defined by the user.

For fitting the smooth and parametric terms, package **mgcv** is employed. For recursive partitioning, package **partykit** is employed. The **gamtree** package is under development; much work and testing still needs to be done. The current version of the package can be installed as follows:

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library("devtools")
install_github("marjoleinF/gamtree")
```



# Examples

First, we load the package:

```{r, warning=FALSE, message=FALSE}
library("gamtree")
```

Next, we load an example dataset to illustrate the functionality of package **gamtree**:

```{r}
data(eco)
summary(eco)
```




```{r, eval = FALSE, echo = FALSE}
## Try binomial response:
eco$succ <- round(eco$Pn) + 2
eco$fail <- 12 - eco$succ
gt.b <- gamtree(cbind(succ, fail) ~ s(PAR) | Species, data = eco,
               cluster = eco$specimen, family = binomial, 
               mob_ctrl = mob_control(ytype = "matrix"))
debug(gamtree)
## this goes wrong because gam gets cbind(succ, fail).succ and cbind(succ, fail).fail as variables

## Try binomial response:
eco$prop <- eco$succ/12
gt.b <- gamtree(prop ~ s(PAR) | Species, data = eco, weights = eco$succ + eco$fail,
               cluster = eco$specimen, family = binomial)

gt.b$gamm$prior.weights
gt.b$data$.weights
plot(gt.b)
## this works
```

The data contain `r nrow(eco)` observations. The `Species` variable is an indicator for plant species. Variable `PAR` will be used as a predictor for the node-specific model, variable `Pn` as the response. Variables `cluster_id` and `noise` are in fact noise variables, but will be used to specify global model terms, to illustrate their use. Finally, `specimen` is an indicator for individual plants.


## Specifying the model formula 

The model is specified through a three- or four-part formula, comprising a response variable, local (subgroup-specific) terms, partitioning variables and global terms. Informally written, a full four-part GAM tree formula can be described as:

```{r, eval = FALSE}
response ~ local terms | global terms | partitioning variables
```

The response must be a single variable; continuous, count, binomial variables are currently supported. The local terms, separated from the response by a tilde (`~`), can comprise one or more smooth and/or parametric terms, as they would be specified in a model fitted with `gam()`. The global terms, separated from the local terms comprise one or more smooth and/or parametric terms, as they would be specified in a model fitted with `gam()`. The partitioning variables, separated from the global terms by a vertical bar(`|`), are specified by providing their names. One can think of this GAM tree formulation as: the effects of the local terms are estimated, conditional on the estimated global terms, conditional on a subgroup structure based on the partitioning variables.

It is not required to specify the global terms: they can simply be omitted by specifying only a two-part left-hand side. This yields a model with an estimated partition (subgroup structure) with subgroup-specific estimates of the (local) terms, without any global terms. The formula can then be described as:

```{r, eval = FALSE}
response ~ local terms | partitioning variables
```




 

## GAM-based recursive partition without global effects

We first fit a GAM-based recursive partition without global effects. We specify `Pn` as the response, with a smooth effect of `PAR`. We specify `Species` as the only potential partitioning variable. Furthermore, we specify the `cluster` argument, which will be passed to the recursive partitioning procedure (function `mob()` from package **partykit**) to indicate that the individual observations in the dataset are not independent, but nested within the different specimen. 

```{r}
gt1 <- gamtree(Pn ~ s(PAR) | Species, data = eco, cluster = eco$specimen)
```

We can inspect the partition by plotting the tree (see `?plot.gamtree` for more info):

```{r, fig.width=5, fig.height=4}
plot(gt1, which = "tree", treeplot_ctrl = list(gp = gpar(cex = .5)))
```

Through the `treeplot_ctrl` argument, we can specify additional argument to be passed to function `plot.party()` (from package **partykit**). We passed the `gp` argument, to have a smaller font size for the node and path labels than with the default `cex = 1`.

The plots indicate similar trajectories in all three terminal nodes, revealing a sharp increase first, which then levels off. The increase appears to level off completely in node 2, while the increase in nodes 4 and 5 only slows down towards the right end. 

Note that the red curves represent the fitted (predicted) values of the observations. They are not very smooth, because they reflect marginal effects, which can be strongly influenced by where data was observed, combined with the effects of other variables. We can also plot conditional effects (i.e., keeping all other predictors fixed) of the predictors:

```{r, fig.width = 5, fig.height = 4}
par(mfrow = c(2, 2))
plot(gt1, which = "terms", which_terms = "local", 
     gamplot_ctrl = list(shade = TRUE, cex.main = .8, cex.axis = .6, cex.lab = .6))
```

We used the `gamplot_ctrl` argument to pass additional arguments to function `plot.gam()` (from package **mgcv**). We specified the `shade` argument, so that the confidence interval are depicted with a grey shaded area. Note however, that the plotted confidence intervals are overly optimistic, because they do not account for the searching of the tree (subgroup) structure.



## GAM-based recursive partition with global effects

We will now include a global part in the fitted model. We add global terms to the earlier `gamtree` model, based on the `noise` and `cluster_id` variables. Both are in fact noise variables, so these should not have significant or substantial effects. They merely serve as an illustration of how to specify a global (i.e., subgroup-invariant) part of the model. We will specify `noise` as having a parametric (i.e., linear) effect and `cluster_id` as an indicator for a random intercept term (which can be fitted using function `s()` and specifying `bs = "re"`). 
 
To estimate both the local and global models, an iterative approach is taken:

- Step 0: Initialize by assuming the global effects to be zero. 

- Step 1: Given the current estimate of the global effects, estimate the partition (subgroup structure).

- Step 2: Given the current estimate of the partition (subgroup structure), estimate the local and global effects.

- Step 3: Repeat steps 1 and 2 until convergence (i.e., change in log-likelihood values from one iteration to the next is smaller than `abstol`).

Note that in Step 0, the assumption of global effects being zero can be replaced with an assumption of there being no subgroups. This can be specified through use of the `globalstart` argument. The only difference is then, that estimation of the model will initialize with Step 2. 
 
```{r}
gt2 <- gamtree(Pn ~ s(PAR) | noise + s(cluster_id, bs="re") | Species,
               data = eco, cluster = eco$specimen)
gt2$iterations
```
Estimation converged in two iterations. Probably because accounting for the global effects has little effect: the predictors for the global model are in fact noise variables, the global effects are thus 0 and the initial estimate of the global effects is already adequate. 

We can obtain test statistics for the significance of the global and local effects in the full GAM using the `summary` method:

```{r}
summary(gt2)
```

Note that the standard errors and degrees of freedom for the smooth and parametric terms in the terminal nodes (i.e., those terms containing `.tree`) do not account for the searching of the tree structure and are therefore likely too low (overly optimistic), yielding too low standard errors and $p$ values.

The effect of the local smooths are significant in every terminal tree node (again, these $p$ values are overly optimistic). As expected, the global fixed and random effects (`noise` and `s(cluster_id)`) are not significant. We will also see this in their estimated random-effects coefficients being close to zero:

```{r}
coef(gt2, which = 'global')
```

Note that by default, the `coef` method returns the local (node-specific) estimates, but we obtained the global coefficient estimates only through specifying the `which` argument (see also `?coef.gamtree`).

We can plot the tree and the models fitted in each of the terminal nodes:

```{r, fig.width=5, fig.height=4}
plot(gt2, which = "tree", treeplot_ctrl = list(gp = gpar(cex = .5)))
par(mfrow = c(2, 2))
plot(gt2, which = "terms", gamplot_ctrl = list(shade = TRUE, cex.main = .8, 
                                               cex.axis = .6, cex.lab = .6))
```





## Evaluating the adequacy of the basis used to represent the smooth terms

The plots indicate that the lines of the fitted smooths are somewhat wiggly, especially in node 2. Perhaps the default dimension for the basis used to represent the smooth term, `k = 9`, may not be adequate for these data. In addition to the above visual inspection of how well the smooths appear to approximate the datapoints, we can use the `gam.check()` function from package **mgcv** to check the adequacy of the dimension of the basis used to represent the smooth term:

```{r, fig.keep = 'none'}
gam.check(gt2$gamm)
```

By default, `gam.check()` also yields residual plots, which are omitted here. The values of `k-index` and `p-value` above indicate that the default `k = 9` may be too low. This is in contrast with the somewhat too wiggly pattern of the smooths we observed in the plots of the smooth terms and datapoints, which suggests the value of `k` may be too high.

Based on the `gam.check()` function results, we could increase the value of `k`, to see if that increases the reported `edf` substantially:

```{r, fig.width=4, fig.height=3}
gt3 <- gamtree(Pn ~ s(PAR, k=18L) | noise + s(cluster_id, bs = "re") | Species, 
               data = eco, cluster = eco$specimen)
plot(gt3, which = "tree", treeplot_ctrl = list(gp = gpar(cex = .5)))
```

We obtained a tree with a single split. Increased flexibility of the smooth curves seems to have accounted for the difference between Eugene and Sapium we saw in the earlier tree. Otherwise, the results seem the same as before: The response variable values appear somewhat lower at the start in node 2, compared to node 3. This difference seems to have increased at the last measurements.

We again apply the `gam.check()` function:

```{r, fig.keep = 'none'}
gam.check(gt3$gamm)
```

The `edf` values have not increased substantially and the `k-index` values are similar to the earlier values. Thus, increasing the number of knots to a larger number than the default seems not necessary. 

We can also *reduce* the value of `k` to see if that yields less wiggly lines, and perhaps a different tree:

```{r, fig.width = 4, fig.height=3}
gt4 <- gamtree(Pn ~ s(PAR, k=5L) | noise + s(cluster_id, bs="re") | Species,
               data = eco, cluster = eco$specimen, verbose = FALSE)
plot(gt4, which = "tree", treeplot_ctrl = list(gp = gpar(cex = .5)))
```


```{r, fig.width = 5, fig.height=4}
par(mfrow = c(2, 2))
plot(gt4, which = "terms", gamplot_ctrl = list(shade = TRUE, cex.main = .8, 
                                               cex.axis = .6, cex.lab = .6))
```

To the eye, a lower value dimension for the bases to represent the smooth terms seems more appropriate; the lower flexibility leads less wiggly lines. The lower value for `k` does not seem to yield a different tree or conclusions anyway. So we will stick with the value of `k = 5`.


## Further inspection of the fitted model

We obtain a summary of the fitted full GAM using the `summary` method:

```{r}
summary(gt4)
```

Note that the standard errors and degrees of freedom for the smooth and parametric terms in the terminal nodes (i.e., those terms containing `.tree`) do not account for the searching of the tree structure and are therefore likely too low (overly optimistic), yielding too low $p$ values.

The `Parameteric coefficients` indicate that the intercepts in every terminal node are significantly different from 0. In light of the standard errors, the differences in intercepts also seem significant. Thus, the starting values appear highest in node 5, and lowest in node 2.

The `Approximate significance of smooth terms` indicate significance of the smooth terms in all three terminal nodes.

Using the `coef` method we can print the coefficients from the terminal nodes:

```{r}
coef(gt4)
```

We can do an additional check on the observation-level contributions to the gradient. We can do this using the `check_grad()` function. It computes the sum of the observation-wise contributions to the gradient. These sums should be reasonably close to zero:

```{r}
check_grad(gt4)
```

The sum of the gradient contributions seem reasonably close to zero.


## Specifying non-default arguments for the partitioning

Let's say would prefer to collapse nodes 4 and 5, because we do not think the differences between the two species are relevant. We can do that through specifying the `maxdepth` argument of function `mob()`, which is used internally by function `gamtree()` to perform the partitioning (splitting). We can pass additional arguments to function `mob()` with the `mob_ctrl` argument:

```{r}
gt5 <- gamtree(Pn ~ s(PAR, k=5L) | Species, data = eco, cluster = eco$specimen, 
               mob_ctrl = mob_control(maxdepth = 2L))
```

Note that function `mob_control()` (from package **partykit**) is used here, to generate a `list` of control arguments for function `mob()`.

We inspect the result:

```{r, fig.width = 4, fig.height=3}
plot(gt5, which = "tree", treeplot_ctrl = list(gp = gpar(cex = .5)))
```

We can again check whether the sums of the observation-wise gradient contributions are reasonably close to zero:

```{r}
check_grad(gt5)
```

the observation-wise gradient contributions sum to values reasonably close to 0.



## Specifying multiple terms and/or predictors for the node-specific GAMs

Multiple predictor variables can be specified for the node-specific model, as is customary with function `gam()`. Parametric as well as non-parametric terms can be specified, both for the global as well as for the local terms. But note that specifying more than a small number of terms for the node-specific models is probably not a good idea; it will yield results which are difficult to interpret (because of the large number of coefficients) and possibly unstable, or may lead to estimation errors. The higher the complexity (i.e., the higher the df for the smooth term, the higher the number of predictor variables) of the node-specific model, the more likely that one or more spurious subgroups will be detected, or actual subgroups may be obfuscated. 

Specifying a predictor for the node-specific model which is known to be noise is not a good idea in the real world, but here we do it anyway for illustration purposes. We add a parametric (i.e., linear) effect of `noise` in the node-specific model:

```{r}
gt6 <- gamtree(Pn ~ s(PAR, k=5L) + noise | s(cluster_id, bs="re") | Species,
               data = eco, cluster = eco$specimen)
summary(gt6)
```

We can also employ different functions than `s()` for the node-specific (or global) GAMs:

```{r}
gt9 <- gamtree(Pn ~ te(PAR, noise) | s(cluster_id, bs="re") | Species,
               data = eco, mob_ctrl = mob_control(maxdepth = 3L))
summary(gt9)
```

